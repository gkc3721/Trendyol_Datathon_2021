import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from sklearn import preprocessing
from sklearn import model_selection
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder

products=pd.read_csv(r"C:\Users\gökçe\Documents\trendyol\products.csv")
transactions= pd.read_csv(r"C:\Users\gökçe\Documents\trendyol\transactions.csv")

transactions.info()
transactions.isna().sum()
df1=transactions.drop(['order_date', 'order_line_item_id','order_parent_id','ship_cost','coupon_id','promotion_name','promotion_award_value'],axis=1)
df1= df1.dropna()
df1.isnull().sum()
transactions.loc[((transactions.original_price- transactions.discounted_price)>0), :]

discount_rate= []
or_pr= df1['original_price']
dis_pr= df1['discounted_price']
a=0
for i in or_pr:
    if i > 0:
        disc_rate= ((i - dis_pr.iloc[a]) / i)*100
    
    else :
        disc_rate= 0
    a=a+1
    discount_rate.append(disc_rate)

df1['Discount_Rate'] = discount_rate
df1= df1.drop(['product_variant_id','discounted_price','coupon_discount'], axis=1)

df2= products.drop(['product_variant_id','product_name','category_name','gender_name',
                    'brand_name','color_name','supplier_color_name', 'attributet_name',
                    'attribute_value'], axis=1)
df2=df2.dropna()

df3= pd.merge(df1, df2, on=['product_content_id'], how='left')
df3.shape
df3.isnull().sum()
df3.corr()

X= df1.drop(['is_returned'], axis=1)
x1= X.loc[:100]

h= df1[['is_returned']].loc[:100]
f=np.ravel(np.array(h),order='F') 
f.shape

y = f
X_train, X_test, y_train, y_test = train_test_split(x1, y, test_size=0.25, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.18, random_state=42)

model = RandomForestClassifier()
rf_model= model.fit(X_train,y_train)
rfc_predict_test = model.predict(X_test)
rfc_predict_train = model.predict(X_train)
predict_val = model.predict(X_val)

print('Training Accuracy:', accuracy_score(y_train, rfc_predict_train))
print('Test Accuracy:', accuracy_score(y_test, rfc_predict_test))
print( 'Validation Accuracy:', accuracy_score(y_val, predict_val))

#feature importances
from matplotlib import cm

feature_list = list(x1.columns)
feature_imp = pd.Series(model.feature_importances_,index=feature_list).sort_values(ascending=True)
top_20 = feature_imp.tail(20)
values = top_20
maxvalues = max(top_20)
cmap = cm.ScalarMappable(cm.colors.Normalize(0, maxvalues), cm.Reds)
plt.yticks(range(len(top_20)), top_20.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.barh(np.arange(len(values)), values, color = cmap.to_rgba(values))
plt.show()

#hyperparameter tuning with grid search
from sklearn.model_selection import GridSearchCV
rfc=RandomForestClassifier(random_state=42)
param_grid = { 
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt'],
    'min_samples_split' : [2, 4],
    'max_depth' : [8,10,11,12],
    'criterion' : ['gini', 'entropy']
}

CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5,n_jobs=-1)
CV_rfc.fit(X_train, y_train)

# Displaying best hyperparameters
rf_grid.best_params
CV_rfc.best_params__

# Accuracy score with new hyperparameters
#
grid_preds = rf_grid.predict(X_val)#
accuracy_score(y_val, grid_pred

rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 300, max_depth=12, min_samples_split= 2)
rfc1.fit(X_train, y_train)
pred=rfc1.predict(X_test)
print("Accuracy for Random Forest on data: ",accuracy_score(y_test,pred))
grid_preds = rfc1.predict(X_val)
accuracy_score(y_val, grid_preds)s)

#Evaluating the performance or the model by confusşon matrix
conf_mat = confusion_matrix(y_test.values.argmax(axis=1), pred.argmax(axis=1))
print(conf_mat)
#Confusion matrix percentages
conf_mat_per = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
conf_mat_per

#Random forest regressor
from sklearn.ensemble import RandomForestRegressor
rf_reg = RandomForestRegressor()
rf_reg.fit(X_train, y_train)
y_pred= rf_reg.predict(X_test)
print("Accuracy on Traing set: ",rf_reg.score(X_train,y_train))
print("Accuracy on Testing set: ",rf_reg.score(X_test,y_test))

from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error

print("\t\tError Table")
print('Mean Absolute Error      : ', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared  Error      : ', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared  Error : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('R Squared Error          : ', metrics.r2_score(y_test, y_pred))
